# í‰ìŠµ ëª©í‘œ : YOLOë¥¼ í™œìš©í•˜ì—¬ ì»´í“¨í„° ë¹„ì „ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•œë‹¤.

# YOLO / RoboFlow

## ëª©ì°¨

1. YOLO
   - YOLOë€?
   - YOLO ëª¨ë¸ë³„ íŠ¹ì§•
   - YOLO11 ì„¤ì¹˜
   - image ì˜ˆì œ ì‹¤ìŠµ
   - ì¹´ë©”ë¼ ìº¡ì²˜ ì˜ˆì œ ì‹¤ìŠµ
   - ì‹ ë¢°ë„(confidence)ì¡°ì ˆ ì˜ˆì œ ì‹¤ìŠµ
   - ì‹¤ì‹œê°„ êµí†µìƒí™© ê°ì§€ ì˜ˆì œ ì‹¤ìŠµ
  
2. ê°œì¸ í”„ë¡œì íŠ¸ (ì›í•˜ëŠ” ê°ì²´ë§Œ íƒì§€)

3. RoboFlow
   - RoboFlowë€
   - íŠœí† ë¦¬ì–¼ í•´ë³´ê¸°

## 1. YOLO

<details>
<summary></summary>
<div markdown="1">

## **1-1. YOLOë€?**

 _You Only Look Once_ ì˜ ì•½ìë¡œ, **ê°ì²´ íƒì§€(Object Detection) ë¶„ì•¼ì—ì„œ ë§ì´ ì“°ì´ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸**

 í•œ ë²ˆì— ì „ì²´ ì´ë¯¸ì§€ë¥¼ ë³´ê³  **ê°ì²´ì˜ ìœ„ì¹˜ì™€ ì¢…ë¥˜ë¥¼ ë™ì‹œì— ì˜ˆì¸¡**í•˜ëŠ” ë°©ì‹

| íŠ¹ì§• | ì„¤ëª… |
|------|------|
| ì†ë„ | í•œ ë²ˆì˜ ì‹ ê²½ë§ ì—°ì‚°ìœ¼ë¡œ ëª¨ë“  ê°ì²´ë¥¼ ê°ì§€í•˜ë¯€ë¡œ ë§¤ìš° ë¹ ë¦„ |
| End-to-End êµ¬ì¡° | ì´ë¯¸ì§€ ì…ë ¥ â†’ ë°”ë¡œ ë°•ìŠ¤ ì¢Œí‘œì™€ í´ë˜ìŠ¤ ì¶œë ¥ |
| ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥ | ì ë‹¹í•œ í•˜ë“œì›¨ì–´ë©´ ì›¹ìº Â·CCTV ë“± ì‹¤ì‹œê°„ ì˜ìƒ ì²˜ë¦¬ ê°€ëŠ¥ |

**[ì‘ë™ ë°©ì‹]**

**1-Stage Detection** : ì˜ì—­ ì¶”ì •(region proposal)"ê³¼ "ë¶„ë¥˜(classification)"ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬

ì „ì²´ ì´ë¯¸ì§€ë¥¼ ê·¸ë¦¬ë“œ(grid)ë¡œ ë‚˜ëˆ ì„œ ê° ì…€(cell)ë§ˆë‹¤ ê°ì²´ë¥¼ íƒì§€

<img width="934" height="877" alt="image" src="https://github.com/user-attachments/assets/6a52c0c5-c944-4ef7-96d7-f364b7b10a98" />

## **1-2. YOLO ëª¨ë¸ë³„ íŠ¹ì§•**

| ëª¨ë¸         | íŠ¹ì§• ìš”ì•½ |
|--------------|----------|
| YOLO v1      | 2-stage â†’ 1-stage ì „í™˜ì˜ ì„ êµ¬ì, ì†ë„ ë¹ ë¦„, ì •í™•ë„ ë‚®ìŒ |
| SSD          | ì†ë„Â·ì„±ëŠ¥ ëª¨ë‘ ìš°ìˆ˜ |
| YOLO v2      | SSDì™€ ì„±ëŠ¥ ë¹„ìŠ·, SSDë³´ë‹¤ ë¹ ë¦„, ì‘ì€ ë¬¼ì²´ íƒì§€ ì„±ëŠ¥ ë‚®ìŒ |
| Retinanet    | YOLO v2ë³´ë‹¤ ëŠë¦¬ì§€ë§Œ ì„±ëŠ¥ ìš°ìˆ˜, FPN ì ìš© |
| YOLO v3      | YOLO v2ë³´ë‹¤ ì†ë„ ì•½ê°„ ëŠë¦¼, ì„±ëŠ¥ í¬ê²Œ í–¥ìƒ, FPN í¬í•¨ |
| EfficientDet | D0~D7 ëª¨ë¸, D0ëŠ” YOLO v3ë³´ë‹¤ ë¹ ë¥´ê³  ì„±ëŠ¥ë„ ì•½ê°„ ìš°ìˆ˜ |

## **1-3. YOLO11 ì„¤ì¹˜**

**[1. YOLO í•™ìŠµìš© ê°€ìƒí™˜ê²½ ìƒì„±]**

```trminal
python -m venv yolovenv
```

_.gitignoreì— yolovenvê°€ìƒí™˜ê²½ ì¶”ê°€_

**[2. YOLO11 ë‹¤ìš´ë¡œë“œ]**

[Ultralytics YOLO11](https://docs.ultralytics.com/ko/models/yolo11/)

```terminal
pip install ultralytics
```

<img width="192" height="74" alt="image" src="https://github.com/user-attachments/assets/50efee40-1e77-41e0-b62c-1784b0ef5ef7" />

## **1-4. image ì˜ˆì œ ì‹¤ìŠµ**

**[1. yolo_image.py ìƒì„±í›„ ì‹¤í–‰]**

```python3
from ultralytics import YOLO

model = YOLO('yolo11n.pt')  # YOLO ë²„ì „ ì§€ì •
```

ì‹¤í–‰ í›„ `yolo11n.pt`ê°€ ìë™ ë‹¤ìš´ë¡œë“œ ë¨

<br><br>

**[2. ì—ì œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ]**

```python3
from ultralytics import YOLO

model = YOLO('yolo11n.pt')

results = model('http://ultralytics.com/images/bus.jpg')

results[0].show()
```

ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ ì´ë¯¸ì§€ ì¶œë ¥

<img width="511" height="680" alt="image" src="https://github.com/user-attachments/assets/e4ae4c5d-36d8-4320-8e7b-c01b8b551f8c" />

**[3. ê°ì²´ ê²€ì¶œ ì½”ë“œ ì¶”ê°€]**

```python3
from ultralytics import YOLO

# YOLO ëª¨ë¸ ì„¤ì •
model = YOLO('yolo11n.pt')

# results = model('http://ultralytics.com/images/bus.jpg')

test_images = [
    'https://ultralytics.com/images/zidan.jpg'
    'https://ultralytics.com/images/bus.jpg'
]

for img in test_images:
    results = model(img)
    print(f'ê²€ì¶œëœ ê°ì²´ ìˆ˜ : {len(results[0].boxes)}')

results[0].show()
```

`bus.jpg` ì´ë¯¸ì§€ì— ëŒ€í•œ ê²°ê³¼ ê°’ ì¶œë ¥

<img width="135" height="22" alt="image" src="https://github.com/user-attachments/assets/cac85cee-f30e-4da9-828b-9099f4f1c730" />

## **1-5. ì¹´ë©”ë¼ ìº¡ì²˜ ì˜ˆì œ ì‹¤ìŠµ**

**[1. ì½”ë“œ ìƒì„±]**

```python3
from ultralytics import YOLO
import cv2

model = YOLO('yolo11n.pt')

cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    if ret:
        results = model(frame, verbose= False)
        annotated_frame = results[0].plot()
        cv2.imshow('Yolo ', annotated_frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
```

**[2. ê²°ê³¼ ì¶œë ¥]**

<img width="638" height="509" alt="image" src="https://github.com/user-attachments/assets/3171e6ba-e6f7-4856-896c-dadcbb9e88d2" />

## **1-6. ì‹ ë¢°ë„(confidence)ì¡°ì ˆ ì˜ˆì œ ì‹¤ìŠµ**

**[1. ì½”ë“œ ìƒì„±]**

```python3
from ultralytics import YOLO
import cv2  

# YOLO ëª¨ë¸ ë¡œë“œ
model = YOLO('yolo11n.pt')

# ì‹ ë¢°ë„ë³„ ê²€ì¶œ ê²°ê³¼ ë¹„êµ
confidence_levels = [0.25, 0.5, 0.75]
test_image = 'https://ultralytics.com/images/bus.jpg'

print("ğŸ§ª ì‹ ë¢°ë„ë³„ ê²€ì¶œ ì‹¤í—˜:")
for conf in confidence_levels:
    results = model(test_image, conf=conf, verbose=False)
    num_objects = len(results[0].boxes) if results[0].boxes else 0
    print(f"ì‹ ë¢°ë„ {conf}: {num_objects}ê°œ ê°ì²´ ê²€ì¶œ")

# ì‹¤ì‹œê°„ ì‹ ë¢°ë„ ì¡°ì • ë„êµ¬
confidence = 0.5
cap = cv2.VideoCapture(0)

print("í‚¤ë³´ë“œ ì¡°ì‘: +/- ë¡œ ì‹ ë¢°ë„ ì¡°ì •, që¡œ ì¢…ë£Œ")
while True:
    ret, frame = cap.read()
    if ret:
        results = model(frame, conf=confidence, verbose=False)
        annotated = results[0].plot()
        
        # í˜„ì¬ ì„¤ì • í‘œì‹œ
        info_text = f"Confidence: {confidence:.2f}"
        cv2.putText(annotated, info_text, (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        
        cv2.imshow('Confidence Tuner', annotated)
    
    key = cv2.waitKey(1) & 0xFF
    if key == ord('q'):
        break
    elif key == ord('+') or key == ord('='):
        confidence = min(0.95, confidence + 0.1)
    elif key == ord('-'):
        confidence = max(0.1, confidence - 0.1)
```

**[2. ê²°ê³¼ ì¶œë ¥]**

<img width="637" height="507" alt="image" src="https://github.com/user-attachments/assets/7bec2c41-c5df-40e1-aa51-4d56143c978c" />

<img width="639" height="507" alt="image" src="https://github.com/user-attachments/assets/f55115d7-a472-4f4e-882b-a639ba85b321" />

## **1-7. ì‹¤ì‹œê°„ êµí†µìƒí™© ê°ì§€ ì˜ˆì œ ì‹¤ìŠµ**

**[1. ì½”ë“œ ìƒì„±]**

```python3
from ultralytics import YOLO
import cv2  

class TrafficMonitor:
    def __init__(self):
        self.model = YOLO('yolo11n.pt')
        self.traffic_classes = {
            0: 'person', 2: 'car', 3: 'motorcycle', 
            5: 'bus', 7: 'truck', 9: 'traffic_light'
        }
        self.stats = {'total_detections': 0, 'by_class': {}}
    
    def analyze_frame(self, frame):
        results = self.model(frame, 
                           classes=list(self.traffic_classes.keys()), 
                           conf=0.5, verbose=False)
        
        frame_stats = {'vehicles': 0, 'pedestrians': 0, 'signals': 0}
        
        if results[0].boxes is not None:
            for box in results[0].boxes:
                class_id = int(box.cls[0])
                class_name = self.traffic_classes[class_id]
                
                if class_id in [2, 3, 5, 7]:  # vehicles
                    frame_stats['vehicles'] += 1
                elif class_id == 0:  # person
                    frame_stats['pedestrians'] += 1
                elif class_id == 9:  # traffic_light
                    frame_stats['signals'] += 1
                
                self.stats['by_class'][class_name] = \
                    self.stats['by_class'].get(class_name, 0) + 1
            
            self.stats['total_detections'] += len(results[0].boxes)
        
        return results[0].plot(), frame_stats
    
    def run_live_monitoring(self):
        cap = cv2.VideoCapture(0)
        print("ğŸš— êµí†µ ëª¨ë‹ˆí„°ë§ ì‹œì‘! që¡œ ì¢…ë£Œ")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            annotated_frame, frame_stats = self.analyze_frame(frame)
            
            # ì •ë³´ í‘œì‹œ
            y = 30
            cv2.putText(annotated_frame, f"Vehicles: {frame_stats['vehicles']}", 
                       (10, y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            y += 30
            cv2.putText(annotated_frame, f"Pedestrians: {frame_stats['pedestrians']}", 
                       (10, y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            y += 30
            cv2.putText(annotated_frame, f"Total Detected: {self.stats['total_detections']}", 
                       (10, y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)
            
            cv2.imshow('Traffic Monitoring System', annotated_frame)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        cap.release()
        cv2.destroyAllWindows()
        self.show_final_stats()
    
    def show_final_stats(self):
        print("\nğŸ“Š ìµœì¢… í†µê³„:")
        print(f"ì´ ê²€ì¶œ íšŸìˆ˜: {self.stats['total_detections']}")
        print("í´ë˜ìŠ¤ë³„ ê²€ì¶œ í˜„í™©:")
        for class_name, count in self.stats['by_class'].items():
            print(f"  {class_name}: {count}íšŒ")

# ì‹œìŠ¤í…œ ì‹¤í–‰
monitor = TrafficMonitor()
monitor.run_live_monitoring()
```

**[2. ê²°ê³¼ ì¶œë ¥]**

<img width="638" height="508" alt="image" src="https://github.com/user-attachments/assets/bad99a11-25db-44d7-98d5-a8fea946da05" />

<img width="636" height="507" alt="image" src="https://github.com/user-attachments/assets/46211b46-f3e3-4f8d-8811-33cddbe24050" />

<img width="161" height="144" alt="image" src="https://github.com/user-attachments/assets/dfb4a25b-c1b0-41c9-b67d-b8f534757057" />

</div>
</details>

## 2. ê°œì¸ í”„ë¡œì íŠ¸ (ì›í•˜ëŠ” ê°ì²´ë§Œ íƒì§€)

<details>
<summary></summary>
<div markdown="1">

## **2-1. ì™„ì„± ì½”ë“œ**

```python3
from ultralytics import YOLO
import cv2

# YOLO ëª¨ë¸ ë¡œë“œ
model = YOLO('yolo11n.pt')

# ë¹„ë””ì˜¤ ì—´ê¸°
cap = cv2.VideoCapture('./video.mp4')

# ì›ë³¸ FPS ê°€ì ¸ì˜¤ê¸°
fps = cap.get(cv2.CAP_PROP_FPS)

start_seconds = 10  # ìë¥¼ ì•ë¶€ë¶„ ì´ˆ
start_frame = int(fps * start_seconds)

# ì•ë¶€ë¶„ í”„ë ˆì„ ë²„ë¦¬ê¸°
for _ in range(start_frame):
    ret = cap.grab()  # í”„ë ˆì„ì„ ì½ê³  ë²„ë¦¼
    if not ret:
        break

# 2ë°° ì†ë„ â†’ ëŒ€ê¸° ì‹œê°„ ì ˆë°˜
delay = max(1, int(1000 / (fps * 2)))

excluded_classes = [0, 72]  # person, refrigerator ì œì™¸
all_classes = list(range(80))
included_classes = [c for c in all_classes if c not in excluded_classes]

# ì´ë¦„ì„ ë°”ê¿€ í´ë˜ìŠ¤ ë²ˆí˜¸ì™€ ëŒ€ì‘ ì´ë¦„ (motorcycle, bicycle â†’ unknown, chair â†’ person)
rename_map = {
    3: "unknown",  # motorcycle
    1: "unknown",  # bicycle
    56: "person"   # chair
}

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # YOLOë¡œ ê°ì²´ íƒì§€ (ì œì™¸ í´ë˜ìŠ¤ ì œì™¸)
    results = model(frame, classes=included_classes)

    # ì›ë³¸ ì´ë¯¸ì§€ ë³µì‚¬
    img = frame.copy()

    # íƒì§€ëœ ë°•ìŠ¤ ì¢Œí‘œ, í´ë˜ìŠ¤ ë²ˆí˜¸, ì‹ ë¢°ë„ ê°€ì ¸ì˜¤ê¸°
    boxes = results[0].boxes.xyxy.cpu().numpy().astype(int)  # (N, 4)
    classes = results[0].boxes.cls.cpu().numpy().astype(int)  # (N,)
    scores = results[0].boxes.conf.cpu().numpy()             # (N,)

    # íƒì§€ëœ ê°ì²´ í•˜ë‚˜ì”© ë°˜ë³µ
    for box, cls, score in zip(boxes, classes, scores):
        x1, y1, x2, y2 = box
        # ê¸°ë³¸ í´ë˜ìŠ¤ëª… ê°€ì ¸ì˜¤ê¸°
        cls_name = model.names[cls]

        # rename_mapì— ìˆìœ¼ë©´ ì´ë¦„ ë°”ê¾¸ê¸°, ì—†ìœ¼ë©´ ê¸°ë³¸ ì´ë¦„ ì‚¬ìš©
        display_name = rename_map.get(cls, cls_name)

        label = f"{display_name} {score:.2f}"

        # ë°•ìŠ¤ì™€ í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
        color = (0, 255, 0)  # ì´ˆë¡ìƒ‰
        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)
        cv2.putText(img, label, (x1, y1 - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

    # í™”ë©´ í‘œì‹œ
    cv2.imshow("YOLO Detection", img)

    # q í‚¤ ëˆ„ë¥´ë©´ ì¢…ë£Œ
    if cv2.waitKey(delay) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
```

## **2-2. ê²°ê³¼ ì´ë¯¸ì§€**

**[1. ì˜ìƒ ì´ˆê¸° ê°ì²´ ì¸ì‹]**

<img width="358" height="667" alt="image" src="https://github.com/user-attachments/assets/b52e4c88-197d-4221-954a-43f0b374c54f" />

<br><br>

**[2. í•„ìš” ì—†ëŠ” ê°ì²´ ì œê±°]**

<img width="360" height="671" alt="image" src="https://github.com/user-attachments/assets/6491824a-c14e-4712-b3e8-ea273803d87b" />

<br><br>

**[3. ê°ì²´ ì´ë¦„ ë³€ê²½ (motorcycle, bicycle â†’ unknown)]**

<img width="368" height="671" alt="image" src="https://github.com/user-attachments/assets/9a758e30-6336-4111-9f57-f5317323a122" />

<br><br>

**[4. ê°ì²´ ì´ë¦„ ë³€ê²½ (chair â†’ person)]**

<img width="358" height="668" alt="image" src="https://github.com/user-attachments/assets/c9e7d77a-ea91-4013-b9e4-8063861e4c88" />

<br><br>

**[5. ì´í›„ ì˜ìƒ ì¶œë ¥]**

<img width="355" height="668" alt="image" src="https://github.com/user-attachments/assets/ca56efdd-50d1-4cb6-a73a-78a5c291a715" />

<img width="358" height="668" alt="image" src="https://github.com/user-attachments/assets/bf39f701-0191-493b-92d8-9327b6b0719d" />

</div>
</details>

## 3. RoboFlow

<details>
<summary></summary>
<div markdown="1">

## **3-1. RoboFlowë€?**

ì´ë¯¸ì§€ ë°ì´í„°ì…‹ ê´€ë¦¬, ë¼ë²¨ë§, ì¦ê°•, ê·¸ë¦¬ê³  ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµê³¼ ë°°í¬ë¥¼ ì‰½ê²Œ í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” í”Œë«í¼

[roboflow](https://roboflow.com/)

## **3-2. íŠœí† ë¦¬ì–¼ í•´ë³´ê¸°**

**[1. íšŒì›ê°€ì… í›„ ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ ë§Œë“¤ê¸°]**

<img width="475" height="475" alt="image" src="https://github.com/user-attachments/assets/1d6f3067-4fb0-4b00-a2ef-25cd874499c1" />

<br><br>

**[2. Counting Screws Computer Vision Model ë‹¤ìš´ë¡œë“œ]**

[ë§í¬](https://universe.roboflow.com/capjamesg/counting-screws/dataset/8)

<img width="475" height="317" alt="image" src="https://github.com/user-attachments/assets/dddf4f2d-bf8d-4ba7-9488-19d55037d717" />

<img width="300" height="261" alt="image" src="https://github.com/user-attachments/assets/763080ad-9818-4754-872a-b8a7152d4f32" />

<img width="400" height="200" alt="image" src="https://github.com/user-attachments/assets/ee9476e0-0cdf-4bc3-9c32-87994ee228ee" />

<br><br>

**[3. ì••ì¶• í•´ì¬ í›„ ì—…ë¡œë“œ]**

<img width="475" height="475" alt="image" src="https://github.com/user-attachments/assets/cc350d3b-aee3-4b24-852a-bb2e894d15ef" />

<br><br>

**[4. ì–´ë…¸í…Œì´ì…˜(Annotations) ì´ë¯¸ì§€ ìˆ˜ì •]**

<img width="475" height="475" alt="image" src="https://github.com/user-attachments/assets/43c2c70d-c9eb-4499-8fe3-540438a10298" />

<br><br>

**[5. ëª¨ë¸ í•™ìŠµ ì‹œí‚¤ê¸°]**

_**Rogoflow Instant Model í´ë¦­**_

<img width="475" height="475" alt="image" src="https://github.com/user-attachments/assets/8dbaf89d-286b-45f2-9ab5-ce5f047f540d" />

<img width="285" height="175" alt="image" src="https://github.com/user-attachments/assets/7e422a9e-892b-4a70-89d6-437c2feaa542" />

<img width="424" height="67" alt="image" src="https://github.com/user-attachments/assets/58466932-8bb2-4ef6-8ad9-9acc29a359a7" />

<img width="475" height="475" alt="image" src="https://github.com/user-attachments/assets/ec8aa95e-488c-4e67-bdf0-2c1eff1d1d07" />

<img width="475" height="440" alt="image" src="https://github.com/user-attachments/assets/51e67c8f-8ae1-46cc-9070-b2cc3e630f6c" />

<br><br>

**[6. ì›Œí¬í”Œë¡œìš°ì—ì„œ ì‹¤í–‰í•˜ê¸°]**

<img width="475" height="475" alt="image" src="https://github.com/user-attachments/assets/43b43222-799a-40aa-9d79-d1aad0459652" />

_ì¶œë ¥ ê²°ê³¼_

<img width="864" height="576" alt="image" src="https://github.com/user-attachments/assets/79ca746c-08f9-4ee7-a175-4bb198e3f7cb" />

</div>
</details>

